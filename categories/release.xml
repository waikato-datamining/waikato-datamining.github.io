<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Data mining (Posts about release)</title><link>https://www.data-mining.co.nz/</link><description></description><atom:link href="https://www.data-mining.co.nz/categories/release.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents Â© 2024 &lt;a href="mailto:fracpete@waikato.ac.nz"&gt;University of Waikato&lt;/a&gt; </copyright><lastBuildDate>Fri, 31 May 2024 02:44:08 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Faster Whisper 1.0.2 (speech-to-text)</title><link>https://www.data-mining.co.nz/news/2024-05-28-faster-whisper/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;New Docker images are now available for speech-to-text using &lt;a class="reference external" href="https://github.com/SYSTRAN/faster-whisper"&gt;Faster Whisper&lt;/a&gt; 1.0.2:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-llm/whisper/tree/main/faster-whisper-1.0.2_cuda12.1"&gt;https://github.com/waikato-llm/whisper/tree/main/faster-whisper-1.0.2_cuda12.1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-llm/whisper/tree/main/faster-whisper-1.0.2_cpu"&gt;https://github.com/waikato-llm/whisper/tree/main/faster-whisper-1.0.2_cpu&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Faster Whisper is a reimplementation of OpenAI's Whisper library with some &lt;a class="reference external" href="https://github.com/SYSTRAN/faster-whisper#benchmark"&gt;dramatic speed ups&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;With the release of these images, the Coqui STT images have been retired (just like the &lt;a class="reference external" href="https://github.com/coqui-ai/STT/blob/main/README.rst"&gt;Coqui STT project itself&lt;/a&gt;).&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2024-05-28-faster-whisper/</guid><pubDate>Mon, 27 May 2024 20:29:00 GMT</pubDate></item><item><title>image-dataset-converter release</title><link>https://www.data-mining.co.nz/news/2024-05-06-idc-release/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;Based on lessons learned from our &lt;a class="reference external" href="https://github.com/waikato-ufdl/wai-annotations"&gt;wai-annotations&lt;/a&gt; library,
we simplified and streamlined the design of a data processing library (though limited to just image datasets).
Of course, it makes use of the latest &lt;a class="reference external" href="https://github.com/waikato-datamining/seppl"&gt;seppl&lt;/a&gt; version, which also
simplified how plugins are being located at runtime and development time.&lt;/p&gt;
&lt;p&gt;The new kid on the block is called &lt;strong&gt;image-dataset-converter&lt;/strong&gt; and its code is located here:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-datamining/image-dataset-converter"&gt;https://github.com/waikato-datamining/image-dataset-converter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Whilst it is based on wai-annotations, it already contains additional functionality.&lt;/p&gt;
&lt;p&gt;And, of course, we also have resources demonstrating how to use the new library:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://www.data-mining.co.nz/image-dataset-converter-examples/"&gt;https://www.data-mining.co.nz/image-dataset-converter-examples/&lt;/a&gt;&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2024-05-06-idc-release/</guid><pubDate>Mon, 06 May 2024 04:12:00 GMT</pubDate></item><item><title>llm-dataset-converter release</title><link>https://www.data-mining.co.nz/news/2024-05-06-ldc-release/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;Version 0.2.3 of our &lt;em&gt;llm-dataset-converter&lt;/em&gt; library is now available.&lt;/p&gt;
&lt;p&gt;Quite a number of changes have happened since the first release last year, like xtuner support,
so check out the full change log here:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter/blob/main/CHANGES.rst"&gt;https://github.com/waikato-llm/llm-dataset-converter/blob/main/CHANGES.rst&lt;/a&gt;&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2024-05-06-ldc-release/</guid><pubDate>Mon, 06 May 2024 01:36:00 GMT</pubDate></item><item><title>XTuner Docker images available</title><link>https://www.data-mining.co.nz/news/2024-04-22-xtuner-docker/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;Docker images for &lt;a class="reference external" href="https://github.com/InternLM/xtuner"&gt;XTuner&lt;/a&gt; 0.1.18 are now available:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;In-house registry:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-xtuner:0.1.18_cuda11.7&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker hub:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;waikatodatamining/pytorch-xtuner:0.1.18_cuda11.7&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;XTuner 0.1.18 now supports the just released llama-3 models (e.g.,
&lt;a class="reference external" href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"&gt;Meta-Llama-3-8B-Instruct&lt;/a&gt;).&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2024-04-22-xtuner-docker/</guid><pubDate>Sun, 21 Apr 2024 23:27:00 GMT</pubDate></item><item><title>MMPretrain 1.2.0 Docker images available</title><link>https://www.data-mining.co.nz/news/2024-03-14-mmpretrain-docker/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;First Docker images are available for the &lt;a class="reference external" href="https://github.com/open-mmlab/mmpretrain"&gt;MMPretrain&lt;/a&gt;
framework, using the 1.2.0 release of MMPretrain (code base as of 2024-01-05):&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-datamining/mmpretrain/tree/master/1.2.0_cuda11.1"&gt;CUDA 11.1&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-datamining/mmpretrain/tree/master/1.2.0_cpu"&gt;CPU&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; MMPretrain is the successor of MMClassification, which can be used for image classification.&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2024-03-14-mmpretrain-docker/</guid><pubDate>Wed, 13 Mar 2024 22:55:00 GMT</pubDate></item><item><title>XTuner Docker images available</title><link>https://www.data-mining.co.nz/news/2024-02-27-xtuner-docker/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;&lt;a class="reference external" href="https://github.com/InternLM/xtuner"&gt;XTuner&lt;/a&gt; is an efficient, flexible and full-featured toolkit for fine-tuning
large models (InternLM, Llama, Baichuan, Qwen, ChatGLM) and released under the Apache 2.0 license. The advantage
of this framework is that it is not tied down to a specific LLM architecture, but supports multiple ones out of the box.
With the just released version &lt;a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter/releases/tag/v0.2.0"&gt;v0.2.0&lt;/a&gt;
of our &lt;a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter"&gt;llm-dataset-converter&lt;/a&gt; Python library,
you can read and write the XTuner JSON format (and apply the usual filtering, of course).&lt;/p&gt;
&lt;p&gt;Here are the newly added image tags:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;In-house registry:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-xtuner:2024-02-19_cuda11.7&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker hub:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;waikatodatamining/pytorch-xtuner:2024-02-19_cuda11.7&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, you can use these Docker images in conjunction with our &lt;a class="reference external" href="https://www.data-mining.co.nz/news/2023-11-03-gifr-release/"&gt;gifr&lt;/a&gt;
Python library for &lt;a class="reference external" href="https://www.gradio.app/"&gt;gradio&lt;/a&gt; interfaces as well (&lt;cite&gt;gifr-textgen&lt;/cite&gt;). Just now we released
version 0.0.4 of the library, which is more flexible in regards to text generation: it can now support send and receive
the conversation history and also parse JSON responses.&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2024-02-27-xtuner-docker/</guid><pubDate>Tue, 27 Feb 2024 03:40:00 GMT</pubDate></item><item><title>Text classification support</title><link>https://www.data-mining.co.nz/news/2024-02-15-text-classification-support/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;Large language models (LLMs) for chatbots are all the rage at the moment, but there is plenty of scope of simpler
tasks like text classification. Requiring less resources and being a lot faster is nice as well.&lt;/p&gt;
&lt;p&gt;We turned the &lt;a class="reference external" href="https://huggingface.co/docs/transformers/v4.36.1/en/tasks/sequence_classification"&gt;HuggingFace example&lt;/a&gt;
for sequence classification into a docker image to make it easy for building such classification models.&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;In-house registry:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.36.0_cuda11.7_classification&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker hub:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;waikatodatamining/pytorch-huggingface-transformers:4.36.0_cuda11.7_classification&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Our &lt;a class="reference external" href="https://github.com/waikato-datamining/gifr"&gt;gifr&lt;/a&gt;
Python library for &lt;a class="reference external" href="https://www.gradio.app/"&gt;gradio&lt;/a&gt; received an interface for text
classification (&lt;cite&gt;gifr-textclass&lt;/cite&gt;) in version 0.0.3.&lt;/p&gt;
&lt;p&gt;The &lt;a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter"&gt;llm-dataset-converter&lt;/a&gt; library
obtained native support for text classification formats with version 0.1.1.&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2024-02-15-text-classification-support/</guid><pubDate>Thu, 15 Feb 2024 03:46:00 GMT</pubDate></item><item><title>Llama-2 Docker images available</title><link>https://www.data-mining.co.nz/news/2023-11-10-llama2-docker/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;Llama-2, despite &lt;a class="reference external" href="https://blog.opensource.org/metas-llama-2-license-is-not-open-source/"&gt;not actually being open-source as advertised&lt;/a&gt;,
is a very powerful large language model (LLM), which can also be fine-tuned with custom data. With
version &lt;a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter/releases/tag/v0.0.3"&gt;v0.0.3&lt;/a&gt;
of our &lt;a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter"&gt;llm-dataset-converter&lt;/a&gt; Python library,
it is now possible to generate data in &lt;a class="reference external" href="https://jsonlines.org/"&gt;jsonlines&lt;/a&gt; format that the new
&lt;a class="reference external" href="https://github.com/waikato-llm/huggingface_transformers/tree/master/4.31.0_cuda11.7_llama2"&gt;Docker images&lt;/a&gt;
for Llama-2 can consume:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;In-house registry:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Docker hub:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;code class="docutils literal"&gt;&lt;span class="pre"&gt;waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2&lt;/span&gt;&lt;/code&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Of course, you can use these Docker images in conjunction with our &lt;a class="reference external" href="https://www.data-mining.co.nz/news/2023-11-03-gifr-release/"&gt;gifr&lt;/a&gt;
Python library for &lt;a class="reference external" href="https://www.gradio.app/"&gt;gradio&lt;/a&gt; interfaces as well (&lt;cite&gt;gifr-textgen&lt;/cite&gt;).&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2023-11-10-llama2-docker/</guid><pubDate>Fri, 10 Nov 2023 03:33:00 GMT</pubDate></item><item><title>gifr release</title><link>https://www.data-mining.co.nz/news/2023-11-03-gifr-release/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;A lot of our Docker images allow the user to make predictions in two ways: using simple
file-polling or via a &lt;a class="reference external" href="https://redis.io/"&gt;Redis&lt;/a&gt; backend. File-polling is great for
testing, but unsuitable for a production system due to wear-and-tear on SSDs.&lt;/p&gt;
&lt;p&gt;Initially, I developed a really simple library for sending and receiving data via Redis,
called &lt;em&gt;simple-redis-helper&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/fracpete/simple-redis-helper"&gt;https://github.com/fracpete/simple-redis-helper&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With this library you get some command-line tools for broadcasting, listening, etc. Sufficient
for someone who is comfortable with the command-line (or especially when logged in remotely
via terminal), but not so great for your clients.&lt;/p&gt;
&lt;p&gt;Now, there is the brilliant &lt;a class="reference external" href="https://www.gradio.app/"&gt;gradio&lt;/a&gt; library that was specifically
developed for such scenarios: to create easy to use and great looking interfaces for your machine
learning models.&lt;/p&gt;
&lt;p&gt;The last couple of days, I have put together a new library that is tailored to our Docker images
called &lt;em&gt;gifr&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-datamining/gifr"&gt;https://github.com/waikato-datamining/gifr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With the first release, the following types of models are supported:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;&lt;p&gt;image classification&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;image segmentation&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;object detection/instance segmentation&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;text generation&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2023-11-03-gifr-release/</guid><pubDate>Fri, 03 Nov 2023 01:00:00 GMT</pubDate></item><item><title>llm-dataset-converter release</title><link>https://www.data-mining.co.nz/news/2023-10-27-ldc-release/</link><dc:creator>University of Waikato</dc:creator><description>&lt;p&gt;Over the last couple of months, we have been working on a little command-line tool that
allows you to convert LLM datasets from one format into another, appropriately called
&lt;cite&gt;llm-dataset-converter&lt;/cite&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter"&gt;https://github.com/waikato-llm/llm-dataset-converter&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;With the first release (0.0.1), you can not only load data from and save to in various formats
(csv/tsv, text, json, jsonlines, parquet). The tool lets you define pipelines using the following format:&lt;/p&gt;
&lt;p&gt;&lt;cite&gt;reader [filter [filter ...]] [writer]&lt;/cite&gt;&lt;/p&gt;
&lt;p&gt;Each component in the pipeline comes with its own set of command-line parameters. You can even &lt;em&gt;tee&lt;/em&gt; off
records and process them differently (e.g., writing the same data to different output formats).&lt;/p&gt;
&lt;p&gt;The library also has other tools, for downloading files or datasets from huggingface or combining text files.&lt;/p&gt;
&lt;p&gt;In order to make building such pipeline-oriented tools simpler to develop, we created a base library
that manages the handling of plugins (and, if necessary, their compatibility) called &lt;cite&gt;seppl&lt;/cite&gt;
(&lt;em&gt;Simple Entry Point PipeLines&lt;/em&gt;):&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://github.com/waikato-datamining/seppl"&gt;https://github.com/waikato-datamining/seppl&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Thanks to seppl, the llm-dataset-converter library can be easily extended with additional modules, as it uses
a dynamic approach to locating plugins: you only need to define in what modules to look for what superclass
(like &lt;cite&gt;Reader&lt;/cite&gt;, &lt;cite&gt;Filter&lt;/cite&gt;, &lt;cite&gt;Writer&lt;/cite&gt;).&lt;/p&gt;</description><category>release</category><guid>https://www.data-mining.co.nz/news/2023-10-27-ldc-release/</guid><pubDate>Thu, 26 Oct 2023 20:47:00 GMT</pubDate></item></channel></rss>