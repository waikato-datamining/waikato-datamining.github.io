<!DOCTYPE html>
<html prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article#
" lang="en">
<head>
<meta charset="utf-8">
<meta name="description" content="Commercial data mining activity at the University of Waikato">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title>Data mining</title>
<link href="../assets/css/all-nocdn.css" rel="stylesheet" type="text/css">
<meta name="theme-color" content="#5670d4">
<meta name="generator" content="Nikola (getnikola.com)">
<link rel="alternate" type="application/rss+xml" title="RSS" hreflang="en" href="../rss.xml">
<link rel="canonical" href="https://www.data-mining.co.nz/news/">
<link rel="next" href="index-5.html" type="text/html">
<!--[if lt IE 9]><script src="../assets/js/html5.js"></script><![endif]-->
</head>
<body>
<a href="#content" class="sr-only sr-only-focusable">Skip to main content</a>

<!-- Menubar -->

<nav class="navbar navbar-expand-md static-top mb-4
navbar-dark
bg-dark
"><div class="container">
<!-- This keeps the margins nice -->
        <a class="navbar-brand" href="../">

            <span id="blog-title">Data mining</span>
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#bs-navbar" aria-controls="bs-navbar" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="bs-navbar">
            <ul class="navbar-nav mr-auto">
<li class="nav-item active">
<a href="." class="nav-link">News <span class="sr-only">(active)</span></a>
                </li>
<li class="nav-item">
<a href="../expertise/" class="nav-link">Expertise</a>
            </li>
<li class="nav-item dropdown">
<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Services</a>
            <div class="dropdown-menu">
                    <a href="../what-we-offer/" class="dropdown-item">What we offer</a>
                    <a href="../s3000/" class="dropdown-item">S3000</a>
                    <a href="https://adams-advisor.cms.waikato.ac.nz/" class="dropdown-item">Calibration advice</a>
            </div>
            </li>
<li class="nav-item dropdown">
<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Resources</a>
            <div class="dropdown-menu">
                    <a href="../software/" class="dropdown-item">Software</a>
                    <a href="../docker-for-data-scientists/" class="dropdown-item">Docker for Data Scientists</a>
                    <a href="../docker-images/" class="dropdown-item">Docker images</a>
                    <a href="../applied-deep-learning/" class="dropdown-item">Applied Deep Learning</a>
            </div>
            </li>
<li class="nav-item dropdown">
<a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">About</a>
            <div class="dropdown-menu">
                    <a href="../people/" class="dropdown-item">People</a>
                    <a href="../clients/" class="dropdown-item">Clients</a>
                    <a href="../contact/" class="dropdown-item">Contact</a>
            </div>

                
            </li>
</ul>
<ul class="navbar-nav navbar-right"></ul>
</div>
<!-- /.navbar-collapse -->
    </div>
<!-- /.container -->
</nav><!-- End of Menubar --><div class="container" id="content" role="main">
    <div class="body-content">
        <!--Body content-->
        
        
        

    


    
<div class="postindex">
    <article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2024-02-27-xtuner-docker/" class="u-url">XTuner Docker images available</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2024-02-27-xtuner-docker/" rel="bookmark">
            <time class="published dt-published" datetime="2024-02-27T16:40:00+13:00" itemprop="datePublished" title="2024-02-27 16:40">2024-02-27 16:40</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p><a class="reference external" href="https://github.com/InternLM/xtuner">XTuner</a> is an efficient, flexible and full-featured toolkit for fine-tuning
large models (InternLM, Llama, Baichuan, Qwen, ChatGLM) and released under the Apache 2.0 license. The advantage
of this framework is that it is not tied down to a specific LLM architecture, but supports multiple ones out of the box.
With the just released version <a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter/releases/tag/v0.2.0">v0.2.0</a>
of our <a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter">llm-dataset-converter</a> Python library,
you can read and write the XTuner JSON format (and apply the usual filtering, of course).</p>
<p>Here are the newly added image tags:</p>
<ul class="simple">
<li>
<p>In-house registry:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-xtuner:2024-02-19_cuda11.7</span></code></p></li>
</ul>
</li>
<li>
<p>Docker hub:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">waikatodatamining/pytorch-xtuner:2024-02-19_cuda11.7</span></code></p></li>
</ul>
</li>
</ul>
<p>Of course, you can use these Docker images in conjunction with our <a class="reference external" href="2023-11-03-gifr-release/">gifr</a>
Python library for <a class="reference external" href="https://www.gradio.app/">gradio</a> interfaces as well (<cite>gifr-textgen</cite>). Just now we released
version 0.0.4 of the library, which is more flexible in regards to text generation: it can now support send and receive
the conversation history and also parse JSON responses.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2024-02-15-text-classification-support/" class="u-url">Text classification support</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2024-02-15-text-classification-support/" rel="bookmark">
            <time class="published dt-published" datetime="2024-02-15T16:46:00+13:00" itemprop="datePublished" title="2024-02-15 16:46">2024-02-15 16:46</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Large language models (LLMs) for chatbots are all the rage at the moment, but there is plenty of scope of simpler
tasks like text classification. Requiring less resources and being a lot faster is nice as well.</p>
<p>We turned the <a class="reference external" href="https://huggingface.co/docs/transformers/v4.36.1/en/tasks/sequence_classification">HuggingFace example</a>
for sequence classification into a docker image to make it easy for building such classification models.</p>
<ul class="simple">
<li>
<p>In-house registry:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.36.0_cuda11.7_classification</span></code></p></li>
</ul>
</li>
<li>
<p>Docker hub:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">waikatodatamining/pytorch-huggingface-transformers:4.36.0_cuda11.7_classification</span></code></p></li>
</ul>
</li>
</ul>
<p>Our <a class="reference external" href="https://github.com/waikato-datamining/gifr">gifr</a>
Python library for <a class="reference external" href="https://www.gradio.app/">gradio</a> received an interface for text
classification (<cite>gifr-textclass</cite>) in version 0.0.3.</p>
<p>The <a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter">llm-dataset-converter</a> library
obtained native support for text classification formats with version 0.1.1.</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-11-10-llama2-docker/" class="u-url">Llama-2 Docker images available</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-11-10-llama2-docker/" rel="bookmark">
            <time class="published dt-published" datetime="2023-11-10T16:33:00+13:00" itemprop="datePublished" title="2023-11-10 16:33">2023-11-10 16:33</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Llama-2, despite <a class="reference external" href="https://blog.opensource.org/metas-llama-2-license-is-not-open-source/">not actually being open-source as advertised</a>,
is a very powerful large language model (LLM), which can also be fine-tuned with custom data. With
version <a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter/releases/tag/v0.0.3">v0.0.3</a>
of our <a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter">llm-dataset-converter</a> Python library,
it is now possible to generate data in <a class="reference external" href="https://jsonlines.org/">jsonlines</a> format that the new
<a class="reference external" href="https://github.com/waikato-llm/huggingface_transformers/tree/master/4.31.0_cuda11.7_llama2">Docker images</a>
for Llama-2 can consume:</p>
<ul class="simple">
<li>
<p>In-house registry:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2</span></code></p></li>
</ul>
</li>
<li>
<p>Docker hub:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_llama2</span></code></p></li>
</ul>
</li>
</ul>
<p>Of course, you can use these Docker images in conjunction with our <a class="reference external" href="2023-11-03-gifr-release/">gifr</a>
Python library for <a class="reference external" href="https://www.gradio.app/">gradio</a> interfaces as well (<cite>gifr-textgen</cite>).</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-11-03-gifr-release/" class="u-url">gifr release</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-11-03-gifr-release/" rel="bookmark">
            <time class="published dt-published" datetime="2023-11-03T14:00:00+13:00" itemprop="datePublished" title="2023-11-03 14:00">2023-11-03 14:00</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>A lot of our Docker images allow the user to make predictions in two ways: using simple
file-polling or via a <a class="reference external" href="https://redis.io/">Redis</a> backend. File-polling is great for
testing, but unsuitable for a production system due to wear-and-tear on SSDs.</p>
<p>Initially, I developed a really simple library for sending and receiving data via Redis,
called <em>simple-redis-helper</em>:</p>
<p><a class="reference external" href="https://github.com/fracpete/simple-redis-helper">https://github.com/fracpete/simple-redis-helper</a></p>
<p>With this library you get some command-line tools for broadcasting, listening, etc. Sufficient
for someone who is comfortable with the command-line (or especially when logged in remotely
via terminal), but not so great for your clients.</p>
<p>Now, there is the brilliant <a class="reference external" href="https://www.gradio.app/">gradio</a> library that was specifically
developed for such scenarios: to create easy to use and great looking interfaces for your machine
learning models.</p>
<p>The last couple of days, I have put together a new library that is tailored to our Docker images
called <em>gifr</em>:</p>
<p><a class="reference external" href="https://github.com/waikato-datamining/gifr">https://github.com/waikato-datamining/gifr</a></p>
<p>With the first release, the following types of models are supported:</p>
<ul class="simple">
<li><p>image classification</p></li>
<li><p>image segmentation</p></li>
<li><p>object detection/instance segmentation</p></li>
<li><p>text generation</p></li>
</ul>
</div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-10-27-ldc-release/" class="u-url">llm-dataset-converter release</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-10-27-ldc-release/" rel="bookmark">
            <time class="published dt-published" datetime="2023-10-27T09:47:00+13:00" itemprop="datePublished" title="2023-10-27 09:47">2023-10-27 09:47</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Over the last couple of months, we have been working on a little command-line tool that
allows you to convert LLM datasets from one format into another, appropriately called
<cite>llm-dataset-converter</cite>:</p>
<p><a class="reference external" href="https://github.com/waikato-llm/llm-dataset-converter">https://github.com/waikato-llm/llm-dataset-converter</a></p>
<p>With the first release (0.0.1), you can not only load data from and save to in various formats
(csv/tsv, text, json, jsonlines, parquet). The tool lets you define pipelines using the following format:</p>
<p><cite>reader [filter [filter ...]] [writer]</cite></p>
<p>Each component in the pipeline comes with its own set of command-line parameters. You can even <em>tee</em> off
records and process them differently (e.g., writing the same data to different output formats).</p>
<p>The library also has other tools, for downloading files or datasets from huggingface or combining text files.</p>
<p>In order to make building such pipeline-oriented tools simpler to develop, we created a base library
that manages the handling of plugins (and, if necessary, their compatibility) called <cite>seppl</cite>
(<em>Simple Entry Point PipeLines</em>):</p>
<p><a class="reference external" href="https://github.com/waikato-datamining/seppl">https://github.com/waikato-datamining/seppl</a></p>
<p>Thanks to seppl, the llm-dataset-converter library can be easily extended with additional modules, as it uses
a dynamic approach to locating plugins: you only need to define in what modules to look for what superclass
(like <cite>Reader</cite>, <cite>Filter</cite>, <cite>Writer</cite>).</p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-10-19-llm-on-github/" class="u-url">LLM organization on Github</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-10-19-llm-on-github/" rel="bookmark">
            <time class="published dt-published" datetime="2023-10-19T10:56:00+13:00" itemprop="datePublished" title="2023-10-19 10:56">2023-10-19 10:56</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>We have launched a new Github organization to house all our libraries and Docker images around
large language models (LLMs):</p>
<p><a class="reference external" href="https://github.com/waikato-llm">https://github.com/waikato-llm</a></p>
    </div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-09-01-mmdetection-docker/" class="u-url">MMDetection 3.1.0 Docker images available</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-09-01-mmdetection-docker/" rel="bookmark">
            <time class="published dt-published" datetime="2023-09-01T16:56:00+12:00" itemprop="datePublished" title="2023-09-01 16:56">2023-09-01 16:56</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>New Docker images are available for the <a class="reference external" href="https://github.com/open-mmlab/mmdetection">MMDetection</a> object detection
framework, using the 3.1.0 release of MMDetection (code base as of 2023-06-30):</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/waikato-datamining/mmdetection/tree/master/3.1.0_cuda11.3">CUDA 11.3</a></p></li>
<li><p><a class="reference external" href="https://github.com/waikato-datamining/mmdetection/tree/master/3.1.0_cpu">CPU</a> (inference only)</p></li>
</ul>
</div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-08-28-finetune-gpt2xl-docker/" class="u-url">Finetune GTP2-XL Docker images available</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-08-28-finetune-gpt2xl-docker/" rel="bookmark">
            <time class="published dt-published" datetime="2023-08-28T15:53:00+12:00" itemprop="datePublished" title="2023-08-28 15:53">2023-08-28 15:53</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>The <a class="reference external" href="https://github.com/Xirider/finetune-gpt2xl">finetune-gpt2xl</a> repository allows the fine-tuning and using of GPT2-XL and GPT-Neo
models (the repository uses the <a class="reference external" href="https://github.com/huggingface/transformers">Hugging Face transformers library</a>)
and is now available via the following <a class="reference external" href="https://github.com/waikato-llm/huggingface_transformers/tree/master/4.7.0_cuda11.1_finetune-gpt2xl_20220924">docker images</a>:</p>
<ul class="simple">
<li>
<p>In-house registry:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924</span></code></p></li>
</ul>
</li>
<li>
<p>Docker hub:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">waikatodatamining/pytorch-huggingface-transformers:4.7.0_cuda11.1_finetune-gpt2xl_20220924</span></code></p></li>
</ul>
</li>
</ul>
</div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-08-28-sam-hq-docker/" class="u-url">Segment-Anything in High Quality Docker images available</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-08-28-sam-hq-docker/" rel="bookmark">
            <time class="published dt-published" datetime="2023-08-28T15:16:00+12:00" itemprop="datePublished" title="2023-08-28 15:16">2023-08-28 15:16</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>Docker images for <a class="reference external" href="https://github.com/SysCV/sam-hq">Segment-Anything in High Quality</a> (SAM-HQ) are now available.</p>
<p>Just like <a class="reference external" href="2023-04-20-sam-docker/">SAM</a>, SAM-HQ is a great tool for aiding a human annotating images for image segmentation or object detection, as it can determine
a relatively good outline of an object based on either a point or a box. Only pre-trained models are available.</p>
<p>The code used by the docker images is available from here:</p>
<p><a class="reference external" href="https://github.com/waikato-datamining/pytorch/tree/master/segment-anything-hq">github.com/waikato-datamining/pytorch/tree/master/segment-anything-hq</a></p>
<p>The tags for the images are as follows:</p>
<ul class="simple">
<li>
<p>In-house registry:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-sam-hq:2023-08-17_cuda11.6</span></code></p></li>
<li><p><code class="docutils literal"><span class="pre">public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-sam-hq:2023-08-17_cpu</span></code></p></li>
</ul>
</li>
<li>
<p>Docker hub:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">waikatodatamining/pytorch-sam-hq:2023-08-17_cuda11.6</span></code></p></li>
<li><p><code class="docutils literal"><span class="pre">waikatodatamining/pytorch-sam-hq:2023-08-17_cpu</span></code></p></li>
</ul>
</li>
</ul>
</div>
    </article><article class="h-entry post-text" itemscope="itemscope" itemtype="http://schema.org/Article"><header><h1 class="p-name entry-title"><a href="2023-08-21-falcontune-docker/" class="u-url">Falcontune Docker images available</a></h1>
        <div class="metadata">
            <p class="byline author vcard"><span class="byline-name fn" itemprop="author">
                University of Waikato
            </span></p>
            <p class="dateline">
            <a href="2023-08-21-falcontune-docker/" rel="bookmark">
            <time class="published dt-published" datetime="2023-08-21T16:39:00+12:00" itemprop="datePublished" title="2023-08-21 16:39">2023-08-21 16:39</time></a>
            </p>
        </div>
    </header><div class="e-content entry-content">
    <p>The <a class="reference external" href="https://github.com/rmihaylov/falcontune">falcontune</a> library for fine-tuning and using Falcon 7B/40B
models (which is based on the <a class="reference external" href="https://github.com/huggingface/transformers">Hugging Face transformers library</a>)
is now available via the following <a class="reference external" href="https://github.com/waikato-datamining/huggingface_transformers/tree/master/4.31.0_cuda11.7_falcontune_20230618">docker images</a>:</p>
<ul class="simple">
<li>
<p>In-house registry:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">public.aml-repo.cms.waikato.ac.nz:443/pytorch/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618</span></code></p></li>
</ul>
</li>
<li>
<p>Docker hub:</p>
<ul>
<li><p><code class="docutils literal"><span class="pre">waikatodatamining/pytorch-huggingface-transformers:4.31.0_cuda11.7_falcontune_20230618</span></code></p></li>
</ul>
</li>
</ul>
</div>
    </article>
</div>

        <ul class="pager postindexpager clearfix">
<li class="next"><a href="index-5.html" rel="next">Older posts</a></li>
        </ul>
<!--End of body content--><footer id="footer">
            Contents © 2024         <a href="mailto:fracpete@waikato.ac.nz">University of Waikato</a> - Powered by         <a href="https://getnikola.com" rel="nofollow">Nikola</a>         
            
            
        </footer>
</div>
</div>


        <script src="../assets/js/all-nocdn.js"></script><script>
    baguetteBox.run('div#content', {
        ignoreClass: 'islink',
        captions: function(element){var i=element.getElementsByTagName('img')[0];return i===undefined?'':i.alt;}});
    </script>
</body>
</html>
